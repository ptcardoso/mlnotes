{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data normalization\n",
    "def normalize(data):\n",
    "    for row in data.T:\n",
    "        r_mean = np.mean(row)\n",
    "        r_range = np.amax(row) - np.amin(row)\n",
    "        \n",
    "        row -= r_mean\n",
    "        row /= r_range\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_backprop(z):\n",
    "    return sigmoid(z) * (np.ones(z.shape) - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    return ez / ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_backprop(z):\n",
    "    return softmax(z) * (np.ones(z.shape) - softmax(z))\n",
    "\n",
    "# cost function\n",
    "def cross_entropy(y_hat, y):\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    cost = np.multiply(y, np.log(y_hat))\n",
    "    cost += np.multiply((np.ones(y.shape) - y), np.log(np.ones(y.shape) - y_hat))\n",
    "    cost *= -1/n\n",
    "    \n",
    "    return cost.sum()\n",
    "\n",
    "# function to get a neuron output\n",
    "def predict(x, w, b, activation='sigmoid'):\n",
    "    z = np.dot(x, w) + b\n",
    "    if activation == 'softmax':\n",
    "        return z, softmax(z)\n",
    "    else:\n",
    "        return z, sigmoid(z)\n",
    "    \n",
    "# examaple layers = [4, 2, 3]\n",
    "def model(layers):\n",
    "    parameters = {}\n",
    "    \n",
    "    network_depth = len(layers) - 1\n",
    "\n",
    "    # generate weights for each layer\n",
    "    for i in range(1, network_depth + 1):\n",
    "        parameters['W%s' % (i - 1)] = np.random.rand(layers[i - 1], layers[i])\n",
    "        parameters['B%s' % (i - 1)] = np.ones((1, layers[i]))\n",
    "        \n",
    "    return parameters, network_depth\n",
    "\n",
    "# forward propagation for neural network\n",
    "def forward_prop(x, parameters, network_depth):\n",
    "    feed = x\n",
    "    caches = []\n",
    "    for i in range(0, network_depth):\n",
    "        linear_cache = (feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "        if i == network_depth - 1:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i], activation='softmax')\n",
    "            activation_cache = (activation, 'softmax')\n",
    "        else:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "            activation_cache = (activation, 'sigmoid')\n",
    "        \n",
    "        caches.append((linear_cache, activation_cache))\n",
    "        \n",
    "        feed = activation\n",
    "    return feed, caches\n",
    "\n",
    "\n",
    "def linear_backprop(dz, linear_cache):\n",
    "    a_prev, w, b = linear_cache\n",
    "    m = a_prev.shape[0]\n",
    "    dw = np.dot(a_prev.T, dz) / m\n",
    "    db = dz.sum(axis=0, keepdims=True) / m\n",
    "    da_prev = np.dot(dz, w.T)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def linear_activation_backprop(da, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_cache[1] == 'sigmoid':\n",
    "        dz = np.multiply(da, sigmoid_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    elif activation_cache[1] == 'softmax':\n",
    "        dz = np.multiply(da, softmax_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def backprop(y_hat, y, caches, parameters, learning_rate=0.05):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # derivative of cross entropy\n",
    "    n_layers = len(caches)\n",
    "    da = (-np.divide(y, y_hat) - np.divide(np.ones(y.shape) - y, np.ones(y.shape) - y_hat))\n",
    "    \n",
    "    for i in list(reversed(range(0, n_layers))):\n",
    "        da_prev, dw, db = linear_activation_backprop(da, caches[i])\n",
    "        update_parameters(dw, db, parameters['W%s' % i], parameters['B%s' % i], learning_rate)\n",
    "        da = da_prev\n",
    "    return parameters\n",
    "    \n",
    "# update parameters function\n",
    "def update_parameters(dw, db, weights, bias, learning_rate=0.05):\n",
    "    weights -= dw * learning_rate\n",
    "    bias -= db * learning_rate\n",
    "\n",
    "# function to train the neural network\n",
    "def train(x, y, iterations, learning_rate=0.05):\n",
    "    parameters, network_depth = model([4,5,3,3])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat, caches = forward_prop(x, parameters, network_depth)\n",
    "        parameters = backprop(y_hat, y, caches, parameters, learning_rate)\n",
    "        if i % 10:\n",
    "            print(y_hat[0, :])\n",
    "            print(y_hat[75, :])\n",
    "            print(y_hat[149, :])\n",
    "            print(\"Loss: {}\".format(cross_entropy(y_hat, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-f935a12532cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-158-5057feb73cfe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, iterations, learning_rate)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-5057feb73cfe>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(y_hat, y, caches, parameters, learning_rate)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mlinear_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# derivative of cross entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "iris_dataset = pd.read_csv('resources/datasets/iris.csv')\n",
    "species = iris_dataset[['species']].values\n",
    "\n",
    "x = normalize(iris_dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values)\n",
    "y = []\n",
    "for s in species.T[0]:\n",
    "    if s == 'setosa':\n",
    "        y.append([1, 0, 0])\n",
    "    elif s == 'versicolor':\n",
    "        y.append([0, 1, 0])\n",
    "    elif s == 'virginica':\n",
    "        y.append([0, 0, 1])\n",
    "print(y)\n",
    "y = np.array(one_hot)\n",
    "\n",
    "train(x, y, 200, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
