{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data normalization\n",
    "def normalize(data):\n",
    "    for row in data.T:\n",
    "        r_mean = np.mean(row)\n",
    "        r_range = np.amax(row) - np.amin(row)\n",
    "        \n",
    "        row -= r_mean\n",
    "        row /= r_range\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_backprop(z):\n",
    "    return sigmoid(z) * (np.ones(z.shape) - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    return ez / ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_backprop(z):\n",
    "    return softmax(z) * (np.ones(z.shape) - softmax(z))\n",
    "\n",
    "# cost function\n",
    "def cross_entropy(y_hat, y):\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    cost = np.multiply(y, np.log(y_hat))\n",
    "    cost += np.multiply((np.ones(y.shape) - y), np.log(np.ones(y.shape) - y_hat))\n",
    "    cost *= -1/n\n",
    "    \n",
    "    return cost.sum()\n",
    "\n",
    "# function to get a neuron output\n",
    "def predict(x, w, b, activation='sigmoid'):\n",
    "    z = np.dot(x, w) + b\n",
    "    if activation == 'softmax':\n",
    "        return z, softmax(z)\n",
    "    else:\n",
    "        return z, sigmoid(z)\n",
    "    \n",
    "# examaple layers = [4, 2, 3]\n",
    "def model(layers):\n",
    "    parameters = {}\n",
    "    \n",
    "    network_depth = len(layers) - 1\n",
    "\n",
    "    # generate weights for each layer\n",
    "    for i in range(1, network_depth + 1):\n",
    "        parameters['W%s' % (i - 1)] = np.random.rand(layers[i - 1], layers[i])\n",
    "        parameters['B%s' % (i - 1)] = np.ones((1, layers[i]))\n",
    "        \n",
    "    return parameters, network_depth\n",
    "\n",
    "# forward propagation for neural network\n",
    "def forward_prop(x, parameters, network_depth):\n",
    "    feed = x\n",
    "    caches = []\n",
    "    for i in range(0, network_depth):\n",
    "        linear_cache = (feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "        if i == network_depth - 1:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i], activation='softmax')\n",
    "            activation_cache = (activation, 'softmax')\n",
    "        else:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "            activation_cache = (activation, 'sigmoid')\n",
    "        \n",
    "        caches.append((linear_cache, activation_cache))\n",
    "        \n",
    "        feed = activation\n",
    "    return feed, caches\n",
    "\n",
    "\n",
    "def linear_backprop(dz, linear_cache):\n",
    "    a_prev, w, b = linear_cache\n",
    "    m = a_prev.shape[0]\n",
    "    dw = np.dot(a_prev.T, dz) / m\n",
    "    db = dz.sum(axis=0, keepdims=True) / m\n",
    "    da_prev = np.dot(dz, w.T)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def linear_activation_backprop(da, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_cache[1] == 'sigmoid':\n",
    "        dz = np.multiply(da, sigmoid_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    elif activation_cache[1] == 'softmax':\n",
    "        dz = np.multiply(da, softmax_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def backprop(y_hat, y, caches, parameters, learning_rate=0.05):    \n",
    "    # derivative of cross entropy\n",
    "    n_layers = len(caches)\n",
    "    da = -(np.divide(y, y_hat) - np.divide(np.ones(y.shape) - y, np.ones(y.shape) - y_hat))\n",
    "    \n",
    "    for i in list(reversed(range(0, n_layers))):\n",
    "        da_prev, dw, db = linear_activation_backprop(da, caches[i])\n",
    "        update_parameters(dw, db, parameters['W%s' % i], parameters['B%s' % i], learning_rate)\n",
    "        da = da_prev\n",
    "    return parameters\n",
    "    \n",
    "# update parameters function\n",
    "def update_parameters(dw, db, weights, bias, learning_rate=0.05):\n",
    "    weights -= dw * learning_rate\n",
    "    bias -= db * learning_rate\n",
    "\n",
    "# function to train the neural network\n",
    "def train(x, y, iterations, learning_rate=0.05):\n",
    "    parameters, network_depth = model([4,10,3])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat, caches = forward_prop(x, parameters, network_depth)\n",
    "        parameters = backprop(y_hat, y, caches, parameters, learning_rate)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss: {}\".format(cross_entropy(y_hat, y)))\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(y_hat[0, :])\n",
    "            print(y_hat[75, :])\n",
    "            print(y_hat[149, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9271477170800648\n",
      "[0.32270382 0.2909261  0.38637008]\n",
      "[0.32654871 0.28880388 0.38464741]\n",
      "[0.33084708 0.2866993  0.38245362]\n",
      "Loss: 1.917063838490444\n",
      "Loss: 1.9135074286005715\n",
      "Loss: 1.910752863214463\n",
      "Loss: 1.9081019158925185\n",
      "Loss: 1.9054711032152531\n",
      "Loss: 1.9028513221311019\n",
      "Loss: 1.900242161173177\n",
      "Loss: 1.8976439081146803\n",
      "Loss: 1.8950567102049463\n",
      "Loss: 1.8924805825989228\n",
      "[0.30508434 0.33449605 0.36041961]\n",
      "[0.2905294  0.34137577 0.36809483]\n",
      "[0.29330011 0.33959918 0.36710071]\n",
      "Loss: 1.889915471906844\n",
      "Loss: 1.8873612942287008\n",
      "Loss: 1.8848179533616114\n",
      "Loss: 1.8822853488250262\n",
      "Loss: 1.8797633792453445\n",
      "Loss: 1.8772519437392634\n",
      "Loss: 1.8747509424610458\n",
      "Loss: 1.8722602768076582\n",
      "Loss: 1.8697798494859994\n",
      "Loss: 1.8673095645252569\n",
      "[0.31274407 0.33506282 0.3521931 ]\n",
      "[0.28456024 0.34364071 0.37179906]\n",
      "[0.2862884  0.34191115 0.37180045]\n",
      "Loss: 1.8648493272677586\n",
      "Loss: 1.8623990443516218\n",
      "Loss: 1.8599586236904542\n",
      "Loss: 1.8575279744521875\n",
      "Loss: 1.8551070070378366\n",
      "Loss: 1.8526956330605122\n",
      "Loss: 1.8502937653247837\n",
      "Loss: 1.8479013178064458\n",
      "Loss: 1.845518205632686\n",
      "Loss: 1.8431443450626617\n",
      "[0.32030682 0.33549594 0.34419724]\n",
      "[0.27877851 0.34582301 0.37539848]\n",
      "[0.2795118  0.34411668 0.37637153]\n",
      "Loss: 1.8407796534684713\n",
      "Loss: 1.8384240493165098\n",
      "Loss: 1.8360774521492105\n",
      "Loss: 1.8337397825671524\n",
      "Loss: 1.83141096221153\n",
      "Loss: 1.8290909137469769\n",
      "Loss: 1.8267795608447335\n",
      "Loss: 1.824476828166148\n",
      "Loss: 1.8221826413465125\n",
      "Loss: 1.8198969269792133\n",
      "[0.3277786  0.33580738 0.33641402]\n",
      "[0.27316558 0.34793488 0.37889955]\n",
      "[0.2729483  0.34622949 0.38082221]\n",
      "Loss: 1.817619612600196\n",
      "Loss: 1.8153506266727384\n",
      "Loss: 1.81308989857252\n",
      "Loss: 1.8108373585729831\n",
      "Loss: 1.8085929378309817\n",
      "Loss: 1.8063565683727028\n",
      "Loss: 1.804128183079868\n",
      "Loss: 1.801907715676193\n",
      "Loss: 1.799695100714111\n",
      "Loss: 1.797490273561751\n",
      "[0.33516591 0.33600652 0.32882757]\n",
      "[0.26770638 0.34998572 0.3823079 ]\n",
      "[0.26657992 0.34826034 0.38515974]\n",
      "Loss: 1.7952931703901591\n",
      "Loss: 1.793103728160768\n",
      "Loss: 1.7909218846130992\n",
      "Loss: 1.7887475782527003\n",
      "Loss: 1.7865807483393064\n",
      "Loss: 1.7844213348752234\n",
      "Loss: 1.7822692785939298\n",
      "Loss: 1.7801245209488865\n",
      "Loss: 1.7779870041025592\n",
      "Loss: 1.7758566709156378\n",
      "[0.34247465 0.33610138 0.32142397]\n",
      "[0.2623878  0.35198357 0.38562863]\n",
      "[0.26039092 0.35021844 0.38939064]\n",
      "Loss: 1.773733464936458\n",
      "Loss: 1.771617330390615\n",
      "Loss: 1.7695082121707664\n",
      "Loss: 1.7674060558266227\n",
      "Loss: 1.7653108075551178\n",
      "Loss: 1.7632224141907578\n",
      "Loss: 1.761140823196142\n",
      "Loss: 1.7590659826526578\n",
      "Loss: 1.7569978412513374\n",
      "Loss: 1.7549363482838825\n",
      "[0.34971018 0.3360988  0.31419102]\n",
      "[0.25719833 0.35393535 0.38886632]\n",
      "[0.25436754 0.35211171 0.39352075]\n",
      "Loss: 1.7528814536338433\n",
      "Loss: 1.750833107767959\n",
      "Loss: 1.7487912617276478\n",
      "Loss: 1.7467558671206458\n",
      "Loss: 1.7447268761127979\n",
      "Loss: 1.7427042414199854\n",
      "Loss: 1.7406879163001991\n",
      "Loss: 1.738677854545749\n",
      "Loss: 1.736674010475607\n",
      "Loss: 1.7346763389278839\n",
      "[0.35687734 0.33600468 0.30711799]\n",
      "[0.25212792 0.35584695 0.39202513]\n",
      "[0.2484977  0.35394695 0.39755535]\n",
      "Loss: 1.7326847952524354\n",
      "Loss: 1.730699335303594\n",
      "Loss: 1.7287199154330262\n",
      "Loss: 1.7267464924827105\n",
      "Loss: 1.7247790237780358\n",
      "Loss: 1.7228174671210144\n",
      "Loss: 1.7208617807836128\n",
      "Loss: 1.7189119235011932\n",
      "Loss: 1.716967854466062\n",
      "Loss: 1.7150295333211325\n",
      "[0.36398052 0.33582408 0.3001954 ]\n",
      "[0.2471677  0.35772349 0.39510881]\n",
      "[0.24277074 0.35573004 0.40149922]\n",
      "Loss: 1.7130969201536876\n",
      "Loss: 1.7111699754892489\n",
      "Loss: 1.709248660285546\n",
      "Loss: 1.7073329359265883\n",
      "Loss: 1.7054227642168296\n",
      "Loss: 1.7035181073754324\n",
      "Loss: 1.7016189280306246\n",
      "Loss: 1.699725189214149\n",
      "Loss: 1.6978368543558022\n",
      "Loss: 1.6959538872780622\n",
      "[0.37102369 0.33556138 0.29341492]\n",
      "[0.24230991 0.35956933 0.39812076]\n",
      "[0.23717731 0.35746603 0.40535666]\n",
      "Loss: 1.6940762521908033\n",
      "Loss: 1.6922039136860958\n",
      "Loss: 1.6903368367330907\n",
      "Loss: 1.688474986672984\n",
      "Loss: 1.686618329214062\n",
      "Loss: 1.6847668304268293\n",
      "Loss: 1.682920456739207\n",
      "Loss: 1.6810791749318146\n",
      "Loss: 1.6792429521333214\n",
      "Loss: 1.6774117558158728\n",
      "[0.37801043 0.33522037 0.2867692 ]\n",
      "[0.23754768 0.36138825 0.40106407]\n",
      "[0.23170914 0.35915932 0.40913154]\n",
      "Loss: 1.6755855537905875\n",
      "Loss: 1.673764314203128\n",
      "Loss: 1.6719480055293365\n",
      "Loss: 1.670136596570941\n",
      "Loss: 1.6683300564513281\n",
      "Loss: 1.6665283546113803\n",
      "Loss: 1.6647314608053776\n",
      "Loss: 1.6629393450969636\n",
      "Loss: 1.6611519778551722\n",
      "Loss: 1.6593693297505157\n",
      "[0.38494396 0.33480433 0.28025172]\n",
      "[0.23287496 0.36318351 0.40394154]\n",
      "[0.22635893 0.36081368 0.41282739]\n",
      "Loss: 1.6575913717511312\n",
      "Loss: 1.6558180751189888\n",
      "Loss: 1.6540494114061546\n",
      "Loss: 1.6522853524511105\n",
      "Loss: 1.6505258703751324\n",
      "Loss: 1.6487709375787187\n",
      "Loss: 1.6470205267380755\n",
      "Loss: 1.6452746108016545\n",
      "Loss: 1.6435331629867393\n",
      "Loss: 1.6417961567760881\n",
      "[0.39182717 0.33431609 0.27385674]\n",
      "[0.22828641 0.36495789 0.4067557 ]\n",
      "[0.22112023 0.36243238 0.41644739]\n",
      "Loss: 1.6400635659146205\n",
      "Loss: 1.6383353644061582\n",
      "Loss: 1.6366115265102108\n",
      "Loss: 1.6348920267388087\n",
      "Loss: 1.6331768398533875\n",
      "Loss: 1.6314659408617107\n",
      "Loss: 1.629759305014844\n",
      "Loss: 1.6280569078041696\n",
      "Loss: 1.626358724958445\n",
      "Loss: 1.6246647324409071\n",
      "[0.39866266 0.33375816 0.26757919]\n",
      "[0.22377732 0.3667138  0.40950888]\n",
      "[0.21598734 0.36401824 0.41999443]\n",
      "Loss: 1.6229749064464132\n",
      "Loss: 1.621289223398629\n",
      "Loss: 1.6196076599472506\n",
      "Loss: 1.6179301929652747\n",
      "Loss: 1.6162567995462997\n",
      "Loss: 1.6145874570018703\n",
      "Loss: 1.6129221428588598\n",
      "Loss: 1.6112608348568869\n",
      "Loss: 1.6096035109457716\n",
      "Loss: 1.6079501492830268\n",
      "[0.40545274 0.33313269 0.26141457]\n",
      "[0.21934354 0.36845331 0.41220316]\n",
      "[0.2109552  0.36557368 0.42347111]\n",
      "Loss: 1.606300728231385\n",
      "Loss: 1.6046552263563594\n",
      "Loss: 1.6030136224238394\n",
      "Loss: 1.6013758953977217\n",
      "Loss: 1.599742024437571\n",
      "Loss: 1.5981119888963171\n",
      "Loss: 1.596485768317982\n",
      "Loss: 1.5948633424354377\n",
      "Loss: 1.5932446911681986\n",
      "Loss: 1.5916297946202405\n",
      "[0.41219946 0.33244161 0.25535893]\n",
      "[0.21498138 0.37017816 0.41484045]\n",
      "[0.20601937 0.36710082 0.42687981]\n",
      "Loss: 1.5900186330778516\n",
      "Loss: 1.5884111870075133\n",
      "Loss: 1.5868074370538099\n",
      "Loss: 1.5852073640373656\n",
      "Loss: 1.5836109489528127\n",
      "Loss: 1.5820181729667833\n",
      "Loss: 1.5804290174159332\n",
      "Loss: 1.5788434638049886\n",
      "Loss: 1.5772614938048204\n",
      "Loss: 1.5756830892505467\n",
      "[0.41890465 0.33168659 0.24940876]\n",
      "[0.21068762 0.37188988 0.41742251]\n",
      "[0.20117588 0.36860144 0.43022268]\n",
      "Loss: 1.574108232139658\n",
      "Loss: 1.572536904630168\n",
      "Loss: 1.5709690890387917\n",
      "Loss: 1.5694047678391467\n",
      "Loss: 1.5678439236599742\n",
      "Loss: 1.566286539283393\n",
      "Loss: 1.564732597643167\n",
      "Loss: 1.5631820818230024\n",
      "Loss: 1.561634975054865\n",
      "Loss: 1.5600912607173194\n",
      "[0.4255699  0.33086912 0.24356097]\n",
      "[0.20645939 0.37358971 0.4199509 ]\n",
      "[0.19642125 0.3700771  0.43350166]\n",
      "Loss: 1.5585509223338905\n",
      "Loss: 1.5570139435714476\n",
      "Loss: 1.5554803082386086\n",
      "Loss: 1.5539500002841646\n",
      "Loss: 1.5524230037955271\n",
      "Loss: 1.5508993029971938\n",
      "Loss: 1.5493788822492363\n",
      "Loss: 1.5478617260458027\n",
      "Loss: 1.5463478190136475\n",
      "Loss: 1.5448371459106731\n",
      "[0.43219661 0.32999054 0.23781285]\n",
      "[0.20229418 0.37527874 0.42242708]\n",
      "[0.19175238 0.37152909 0.43671853]\n",
      "Loss: 1.543329691624494\n",
      "Loss: 1.541825441171019\n",
      "Loss: 1.540324379693052\n",
      "Loss: 1.5388264924589083\n",
      "Loss: 1.537331764861053\n",
      "Loss: 1.5358401824147525\n",
      "Loss: 1.5343517307567458\n",
      "Loss: 1.5328663956439321\n",
      "Loss: 1.5313841629520755\n",
      "Loss: 1.5299050186745249\n",
      "[0.43878599 0.32905202 0.23216199]\n",
      "[0.19818979 0.37695785 0.42485236]\n",
      "[0.18716655 0.37295855 0.4398749 ]\n",
      "Loss: 1.5284289489209517\n",
      "Loss: 1.5269559399161021\n",
      "Loss: 1.5254859779985668\n",
      "Loss: 1.5240190496195645\n",
      "Loss: 1.522555141341741\n",
      "Loss: 1.5210942398379856\n",
      "Loss: 1.5196363318902582\n",
      "Loss: 1.5181814043884363\n",
      "Loss: 1.5167294443291712\n",
      "Loss: 1.5152804388147616\n",
      "[0.44533907 0.32805466 0.22660627]\n",
      "[0.19414429 0.37862777 0.42722794]\n",
      "[0.18266135 0.37436641 0.44297224]\n",
      "Loss: 1.5138343750520402\n",
      "Loss: 1.5123912403512767\n",
      "Loss: 1.5109510221250884\n",
      "Loss: 1.50951370788737\n",
      "Loss: 1.5080792852522351\n",
      "Loss: 1.5066477419329694\n",
      "Loss: 1.505219065740998\n",
      "Loss: 1.5037932445848652\n",
      "Loss: 1.5023702664692262\n",
      "Loss: 1.5009501194938522\n",
      "[0.45185672 0.32699944 0.22114383]\n",
      "[0.19015598 0.38028909 0.42955493]\n",
      "[0.17823465 0.37575347 0.44601188]\n",
      "Loss: 1.4995327918526473\n",
      "Loss: 1.4981182718326767\n",
      "Loss: 1.4967065478132069\n",
      "Loss: 1.4952976082647595\n",
      "Loss: 1.4938914417481732\n",
      "Loss: 1.492488036913681\n",
      "Loss: 1.4910873824999935\n",
      "Loss: 1.4896894673333998\n",
      "Loss: 1.4882942803268726\n",
      "Loss: 1.4869018104791896\n",
      "[0.45833968 0.32588727 0.21577304]\n",
      "[0.18622339 0.38194228 0.43183434]\n",
      "[0.17388459 0.37712037 0.44899504]\n",
      "Loss: 1.4855120468740615\n",
      "Loss: 1.4841249786792732\n",
      "Loss: 1.4827405951458332\n",
      "Loss: 1.481358885607135\n",
      "Loss: 1.479979839478127\n",
      "Loss: 1.4786034462544928\n",
      "Loss: 1.4772296955118427\n",
      "Loss: 1.4758585769049124\n",
      "Loss: 1.4744900801667722\n",
      "Loss: 1.4731241951080465\n",
      "[0.46478854 0.324719   0.21049246]\n",
      "[0.18234523 0.38358769 0.43406708]\n",
      "[0.1696095  0.37846766 0.45192284]\n",
      "Loss: 1.471760911616142\n",
      "Loss: 1.4704002196544836\n",
      "Loss: 1.4690421092617618\n",
      "Loss: 1.4676865705511866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4663335937097517\n",
      "Loss: 1.4649831689975077\n",
      "Loss: 1.4636352867468414\n",
      "Loss: 1.4622899373617675\n",
      "Loss: 1.4609471113172234\n",
      "Loss: 1.4596067991583772\n",
      "[0.47120376 0.32349543 0.20530081]\n",
      "[0.1785204 0.3852256 0.436254 ]\n",
      "[0.16540794 0.37979578 0.45479629]\n",
      "Loss: 1.4582689914999416\n",
      "Loss: 1.4569336790254943\n",
      "Loss: 1.45560085248681\n",
      "Loss: 1.4542705027031961\n",
      "Loss: 1.4529426205608391\n",
      "Loss: 1.4516171970121579\n",
      "Loss: 1.4502942230751614\n",
      "Loss: 1.4489736898328212\n",
      "Loss: 1.447655588432442\n",
      "Loss: 1.4463399100850474\n",
      "[0.47758571 0.32221732 0.20019697]\n",
      "[0.17474794 0.38685616 0.43839589]\n",
      "[0.16127862 0.38110506 0.45761632]\n",
      "Loss: 1.4450266460647634\n",
      "Loss: 1.443715787708224\n",
      "Loss: 1.4424073264139659\n",
      "Loss: 1.4411012536418444\n",
      "Loss: 1.4397975609124454\n",
      "Loss: 1.438496239806513\n",
      "Loss: 1.4371972819643777\n",
      "Loss: 1.435900679085393\n",
      "Loss: 1.434606422927379\n",
      "Loss: 1.433314505306071\n",
      "[0.48393461 0.32088541 0.19517997]\n",
      "[0.17102704 0.38847949 0.44049347]\n",
      "[0.15722043 0.38239577 0.4603838 ]\n",
      "Loss: 1.4320249180945754\n",
      "Loss: 1.4307376532228313\n",
      "Loss: 1.4294527026770791\n",
      "Loss: 1.428170058499332\n",
      "Loss: 1.4268897127868594\n",
      "Loss: 1.4256116576916684\n",
      "Loss: 1.4243358854199992\n",
      "Loss: 1.423062388231819\n",
      "Loss: 1.4217911584403269\n",
      "Loss: 1.4205221884114596\n",
      "[0.49025064 0.31950043 0.19024893]\n",
      "[0.167357   0.39009561 0.44254739]\n",
      "[0.15323237 0.38366812 0.46309951]\n",
      "Loss: 1.4192554705634093\n",
      "Loss: 1.4179909973661395\n",
      "Loss: 1.4167287613409107\n",
      "Loss: 1.415468755059811\n",
      "Loss: 1.4142109711452904\n",
      "Loss: 1.4129554022697015\n",
      "Loss: 1.411702041154845\n",
      "Loss: 1.4104508805715206\n",
      "Loss: 1.4092019133390812\n",
      "Loss: 1.407955132324994\n",
      "[0.49653385 0.31806308 0.18540308]\n",
      "[0.16373723 0.39170449 0.44455828]\n",
      "[0.14931358 0.38492224 0.46576419]\n",
      "Loss: 1.4067105304444074\n",
      "Loss: 1.4054681006597185\n",
      "Loss: 1.4042278359801486\n",
      "Loss: 1.4029897294613238\n",
      "Loss: 1.4017537742048587\n",
      "Loss: 1.4005199633579437\n",
      "Loss: 1.3992882901129406\n",
      "Loss: 1.3980587477069772\n",
      "Loss: 1.3968313294215533\n",
      "Loss: 1.3956060285821434\n",
      "[0.50278422 0.31657407 0.18064172]\n",
      "[0.16016725 0.39330604 0.44652671]\n",
      "[0.14546327 0.38615822 0.46837851]\n",
      "Loss: 1.39438283855781\n",
      "Loss: 1.3931617527608164\n",
      "Loss: 1.3919427646462488\n",
      "Loss: 1.390725867711636\n",
      "Loss: 1.38951105549658\n",
      "Loss: 1.388298321582386\n",
      "Loss: 1.3870876595916972\n",
      "Loss: 1.385879063188137\n",
      "Loss: 1.3846725260759476\n",
      "Loss: 1.3834680419996424\n",
      "[0.50900166 0.31503411 0.17596423]\n",
      "[0.15664663 0.39490015 0.44845321]\n",
      "[0.14168078 0.38737611 0.47094311]\n",
      "Loss: 1.3822656047436532\n",
      "Loss: 1.3810652081319872\n",
      "Loss: 1.3798668460278838\n",
      "Loss: 1.378670512333478\n",
      "Loss: 1.377476200989466\n",
      "Loss: 1.3762839059747733\n",
      "Loss: 1.3750936213062306\n",
      "Loss: 1.3739053410382456\n",
      "Loss: 1.372719059262488\n",
      "Loss: 1.3715347701075693\n",
      "[0.51518602 0.31344393 0.17137005]\n",
      "[0.15317506 0.39648665 0.4503383 ]\n",
      "[0.1379655  0.38857591 0.47345859]\n",
      "Loss: 1.37035246773873\n",
      "Loss: 1.369172146357532\n",
      "Loss: 1.3679938002015484\n",
      "Loss: 1.3668174235440633\n",
      "Loss: 1.3656430106937694\n",
      "Loss: 1.3644705559944728\n",
      "Loss: 1.3633000538247968\n",
      "Loss: 1.3621314985978943\n",
      "Loss: 1.3609648847611573\n",
      "Loss: 1.3598002067959343\n",
      "[0.52133708 0.31180426 0.16685865]\n",
      "[0.14975225 0.39806532 0.45218243]\n",
      "[0.13431689 0.38975761 0.47592549]\n",
      "Loss: 1.358637459217249\n",
      "Loss: 1.357476636573518\n",
      "Loss: 1.3563177334462804\n",
      "Loss: 1.3551607444499212\n",
      "Loss: 1.3540056642314027\n",
      "Loss: 1.352852487469999\n",
      "Loss: 1.3517012088770277\n",
      "Loss: 1.3505518231955933\n",
      "Loss: 1.3494043252003247\n",
      "Loss: 1.348258709697122\n",
      "[0.52745457 0.31011586 0.16242957]\n",
      "[0.14637801 0.39963593 0.45398606]\n",
      "[0.13073447 0.39092117 0.47834435]\n",
      "Loss: 1.347114971522898\n",
      "Loss: 1.345973105545337\n",
      "Loss: 1.344833106662636\n",
      "Loss: 1.3436949698032667\n",
      "Loss: 1.3425586899257294\n",
      "Loss: 1.3414242620183123\n",
      "Loss: 1.3402916810988557\n",
      "Loss: 1.339160942214514\n",
      "Loss: 1.3380320404415258\n",
      "Loss: 1.3369049708849787\n",
      "[0.53353816 0.30837949 0.15808235]\n",
      "[0.14305215 0.40119823 0.45574961]\n",
      "[0.12721781 0.39206652 0.48071567]\n",
      "Loss: 1.3357797286785864\n",
      "Loss: 1.3346563089844585\n",
      "Loss: 1.333534706992881\n",
      "Loss: 1.332414917922089\n",
      "Loss: 1.3312969370180547\n",
      "Loss: 1.3301807595542665\n",
      "Loss: 1.3290663808315166\n",
      "Loss: 1.3279537961776857\n",
      "Loss: 1.3268430009475356\n",
      "Loss: 1.3257339905225023\n",
      "[0.5395875  0.30659595 0.15381655]\n",
      "[0.13977457 0.40275194 0.45747349]\n",
      "[0.1237665  0.39319357 0.48303993]\n",
      "Loss: 1.3246267603104855\n",
      "Loss: 1.3235213057456503\n",
      "Loss: 1.3224176222882216\n",
      "Loss: 1.3213157054242872\n",
      "Loss: 1.3202155506655981\n",
      "Loss: 1.3191171535493733\n",
      "Loss: 1.3180205096381092\n",
      "Loss: 1.3169256145193842\n",
      "Loss: 1.3158324638056693\n",
      "Loss: 1.3147410531341441\n",
      "[0.54560219 0.30476606 0.14963175]\n",
      "[0.13654516 0.40429674 0.45915809]\n",
      "[0.12038016 0.39430224 0.48531759]\n",
      "Loss: 1.3136513781665065\n",
      "Loss: 1.3125634345887915\n",
      "Loss: 1.31147721811119\n",
      "Loss: 1.3103927244678637\n",
      "Loss: 1.3093099494167721\n",
      "Loss: 1.3082288887394933\n",
      "Loss: 1.3071495382410476\n",
      "Loss: 1.3060718937497295\n",
      "Loss: 1.3049959511169298\n",
      "Loss: 1.3039217062169712\n",
      "[0.55158178 0.30289067 0.14552755]\n",
      "[0.13336388 0.40583234 0.46080378]\n",
      "[0.11705846 0.39539244 0.4875491 ]\n",
      "Loss: 1.302849154946938\n",
      "Loss: 1.3017782932265092\n",
      "Loss: 1.3007091169977956\n",
      "Loss: 1.2996416222251752\n",
      "Loss: 1.2985758048951337\n",
      "Loss: 1.2975116610161024\n",
      "Loss: 1.2964491866183023\n",
      "Loss: 1.2953883777535855\n",
      "Loss: 1.2943292304952811\n",
      "Loss: 1.2932717409380419\n",
      "[0.55752581 0.30097066 0.14150352]\n",
      "[0.13023067 0.40735839 0.46241094]\n",
      "[0.11380105 0.39646405 0.4897349 ]\n",
      "Loss: 1.2922159051976925\n",
      "Loss: 1.2911617194110776\n",
      "Loss: 1.2901091797359143\n",
      "Loss: 1.2890582823506431\n",
      "Loss: 1.2880090234542838\n",
      "Loss: 1.286961399266289\n",
      "Loss: 1.285915406026401\n",
      "Loss: 1.2848710399945107\n",
      "Loss: 1.283828297450518\n",
      "Loss: 1.2827871746941903\n",
      "[0.5634338  0.29900694 0.13755926]\n",
      "[0.12714553 0.40887457 0.46397991]\n",
      "[0.11060761 0.39751699 0.4918754 ]\n",
      "Loss: 1.2817476680450244\n",
      "Loss: 1.280709773842115\n",
      "Loss: 1.2796734884440142\n",
      "Loss: 1.2786388082286004\n",
      "Loss: 1.2776057295929448\n",
      "Loss: 1.2765742489531804\n",
      "Loss: 1.2755443627443723\n",
      "Loss: 1.27451606742039\n",
      "Loss: 1.273489359453778\n",
      "Loss: 1.272464235335629\n",
      "[0.56930523 0.29700044 0.13369432]\n",
      "[0.12410843 0.41038053 0.46551104]\n",
      "[0.10747782 0.39855114 0.49397103]\n",
      "Loss: 1.2714406915754648\n",
      "Loss: 1.2704187247011032\n",
      "Loss: 1.269398331258543\n",
      "Loss: 1.2683795078118405\n",
      "Loss: 1.2673622509429858\n",
      "Loss: 1.2663465572517894\n",
      "Loss: 1.2653324233557597\n",
      "Loss: 1.2643198458899876\n",
      "Loss: 1.2633088215070312\n",
      "Loss: 1.2622993468768007\n",
      "[0.57513958 0.29495215 0.12990828]\n",
      "[0.12111939 0.41187592 0.46700469]\n",
      "[0.10441136 0.39956642 0.49602221]\n",
      "Loss: 1.261291418686445\n",
      "Loss: 1.2602850336402394\n",
      "Loss: 1.2592801884594724\n",
      "Loss: 1.2582768798823385\n",
      "Loss: 1.2572751046638257\n",
      "Loss: 1.2562748595756088\n",
      "Loss: 1.2552761414059437\n",
      "Loss: 1.254278946959555\n",
      "Loss: 1.253283273057538\n",
      "Loss: 1.252289116537252\n",
      "[0.58093628 0.29286305 0.12620067]\n",
      "[0.1181784 0.4133604 0.4684612]\n",
      "[0.10140792 0.40056274 0.49802935]\n",
      "Loss: 1.2512964742522137\n",
      "Loss: 1.250305343071998\n",
      "Loss: 1.2493157198821363\n",
      "Loss: 1.248327601584015\n",
      "Loss: 1.247340985094777\n",
      "Loss: 1.246355867347221\n",
      "Loss: 1.2453722452897076\n",
      "Loss: 1.2443901158860573\n",
      "Loss: 1.243409476115459\n",
      "Loss: 1.2424303229723717\n",
      "[0.5866948  0.29073418 0.12257102]\n",
      "[0.11528549 0.41483362 0.46988089]\n",
      "[0.09846714 0.40154    0.49999285]\n",
      "Loss: 1.2414526534664327\n",
      "Loss: 1.2404764646223636\n",
      "Loss: 1.2395017534798756\n",
      "Loss: 1.2385285170935814\n",
      "Loss: 1.237556752532903\n",
      "Loss: 1.2365864568819784\n",
      "Loss: 1.235617627239578\n",
      "Loss: 1.2346502607190124\n",
      "Loss: 1.2336843544480454\n",
      "Loss: 1.232719905568808\n",
      "[0.59241456 0.28856661 0.11901883]\n",
      "[0.11244064 0.41629523 0.47126412]\n",
      "[0.09558871 0.40249815 0.50191314]\n",
      "Loss: 1.231756911237713\n",
      "Loss: 1.2307953686253674\n",
      "Loss: 1.2298352749164885\n",
      "Loss: 1.2288766273098224\n",
      "Loss: 1.2279194230180588\n",
      "Loss: 1.2269636592677498\n",
      "Loss: 1.2260093332992277\n",
      "Loss: 1.2250564423665216\n",
      "Loss: 1.2241049837372842\n",
      "Loss: 1.2231549546927063\n",
      "[0.59809498 0.28636144 0.11554358]\n",
      "[0.10964387 0.4177449  0.47261123]\n",
      "[0.09277225 0.40343713 0.50379062]\n",
      "Loss: 1.2222063525274394\n",
      "Loss: 1.2212591745495178\n",
      "Loss: 1.220313418080285\n",
      "Loss: 1.2193690804543085\n",
      "Loss: 1.2184261590193133\n",
      "Loss: 1.2174846511361008\n",
      "Loss: 1.216544554178474\n",
      "Loss: 1.2156058655331674\n",
      "Loss: 1.2146685825997685\n",
      "Loss: 1.2137327027906508\n",
      "[0.6037355  0.28411979 0.11214472]\n",
      "[0.10689517 0.41918228 0.47392255]\n",
      "[0.09001739 0.4043569  0.50562571]\n",
      "Loss: 1.2127982235308945\n",
      "Loss: 1.211865142258223\n",
      "Loss: 1.2109334564229255\n",
      "Loss: 1.2100031634877881\n",
      "Loss: 1.2090742609280265\n",
      "Loss: 1.2081467462312157\n",
      "Loss: 1.2072206168972186\n",
      "Loss: 1.2062958704381228\n",
      "Loss: 1.2053725043781705\n",
      "Loss: 1.20445051625369\n",
      "[0.60933552 0.28184281 0.10882167]\n",
      "[0.10419452 0.42060705 0.47519843]\n",
      "[0.08732376 0.40525742 0.50741883]\n",
      "Loss: 1.2035299036130334\n",
      "Loss: 1.2026106640165057\n",
      "Loss: 1.201692795036308\n",
      "Loss: 1.2007762942564622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1998611592727522\n",
      "Loss: 1.1989473876926637\n",
      "Loss: 1.198034977135316\n",
      "Loss: 1.1971239252313985\n",
      "Loss: 1.1962142296231137\n",
      "Loss: 1.1953058879641127\n",
      "[0.61489447 0.27953171 0.10557382]\n",
      "[0.10154189 0.42201889 0.47643922]\n",
      "[0.08469092 0.40613867 0.5091704 ]\n",
      "Loss: 1.1943988979194333\n",
      "Loss: 1.1934932571654417\n",
      "Loss: 1.192588963389771\n",
      "Loss: 1.1916860142912598\n",
      "Loss: 1.1907844075798981\n",
      "Loss: 1.189884140976764\n",
      "Loss: 1.188985212213967\n",
      "Loss: 1.18808761903459\n",
      "Loss: 1.1871913591926337\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = pd.read_csv('resources/datasets/iris.csv')\n",
    "species = iris_dataset[['species']].values\n",
    "\n",
    "x = normalize(iris_dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values)\n",
    "y = []\n",
    "for s in species.T[0]:\n",
    "    if s == 'setosa':\n",
    "        y.append([1, 0, 0])\n",
    "    elif s == 'versicolor':\n",
    "        y.append([0, 1, 0])\n",
    "    elif s == 'virginica':\n",
    "        y.append([0, 0, 1])\n",
    "y = np.array(y)\n",
    "\n",
    "train(x, y, 5000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
