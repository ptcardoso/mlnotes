{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data normalization\n",
    "def normalize(data):\n",
    "    for row in data.T:\n",
    "        r_mean = np.mean(row)\n",
    "        r_range = np.amax(row) - np.amin(row)\n",
    "        \n",
    "        row -= r_mean\n",
    "        row /= r_range\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_backprop(z):\n",
    "    return sigmoid(z) * (np.ones(z.shape) - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    return ez / ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_backprop(z):\n",
    "    return softmax(z) * (np.ones(z.shape) - softmax(z))\n",
    "\n",
    "# cost function\n",
    "def cross_entropy(y_hat, y):\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    cost = np.multiply(y, np.log(y_hat))\n",
    "    cost += np.multiply((np.ones(y.shape) - y), np.log(np.ones(y.shape) - y_hat))\n",
    "    cost *= -1/n\n",
    "    \n",
    "    return cost.sum()\n",
    "\n",
    "# function to get a neuron output\n",
    "def predict(x, w, b, activation='sigmoid'):\n",
    "    z = np.dot(x, w) + b\n",
    "    if activation == 'softmax':\n",
    "        return z, softmax(z)\n",
    "    else:\n",
    "        return z, sigmoid(z)\n",
    "    \n",
    "# examaple layers = [4, 2, 3]\n",
    "def model(layers):\n",
    "    parameters = {}\n",
    "    \n",
    "    network_depth = len(layers) - 1\n",
    "\n",
    "    # generate weights for each layer\n",
    "    for i in range(1, network_depth + 1):\n",
    "        parameters['W%s' % (i - 1)] = np.random.rand(layers[i - 1], layers[i])\n",
    "        parameters['B%s' % (i - 1)] = np.ones((1, layers[i]))\n",
    "        \n",
    "    return parameters, network_depth\n",
    "\n",
    "# forward propagation for neural network\n",
    "def forward_prop(x, parameters, network_depth):\n",
    "    feed = x\n",
    "    caches = []\n",
    "    for i in range(0, network_depth):\n",
    "        linear_cache = (feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "        if i == network_depth - 1:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i], activation='softmax')\n",
    "            activation_cache = (activation, 'softmax')\n",
    "        else:\n",
    "            linear, activation = predict(feed, parameters['W%s' % i], parameters['B%s' % i])\n",
    "            activation_cache = (activation, 'sigmoid')\n",
    "        \n",
    "        caches.append((linear_cache, activation_cache))\n",
    "        \n",
    "        feed = activation\n",
    "    return feed, caches\n",
    "\n",
    "\n",
    "def linear_backprop(dz, linear_cache):\n",
    "    a_prev, w, b = linear_cache\n",
    "    m = a_prev.shape[0]\n",
    "    dw = np.dot(a_prev.T, dz) / m\n",
    "    db = dz.sum(axis=0, keepdims=True) / m\n",
    "    da_prev = np.dot(dz, w.T)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def linear_activation_backprop(da, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_cache[1] == 'sigmoid':\n",
    "        dz = np.multiply(da, sigmoid_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    elif activation_cache[1] == 'softmax':\n",
    "        dz = np.multiply(da, softmax_backprop(activation_cache[0]))\n",
    "        da_prev, dw, db = linear_backprop(dz, linear_cache)\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "def backprop(y_hat, y, caches, parameters, learning_rate=0.05):    \n",
    "    # derivative of cross entropy\n",
    "    n_layers = len(caches)\n",
    "    da = -(np.divide(y, y_hat) - np.divide(np.ones(y.shape) - y, np.ones(y.shape) - y_hat))\n",
    "    \n",
    "    for i in list(reversed(range(0, n_layers))):\n",
    "        da_prev, dw, db = linear_activation_backprop(da, caches[i])\n",
    "        update_parameters(dw, db, parameters['W%s' % i], parameters['B%s' % i], learning_rate)\n",
    "        da = da_prev\n",
    "    return parameters\n",
    "    \n",
    "# update parameters function\n",
    "def update_parameters(dw, db, weights, bias, learning_rate=0.05):\n",
    "    weights -= dw * learning_rate\n",
    "    bias -= db * learning_rate\n",
    "\n",
    "# function to train the neural network\n",
    "def train(x, y, iterations, learning_rate=0.05):\n",
    "    parameters, network_depth = model([4,12,3])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_hat, caches = forward_prop(x, parameters, network_depth)\n",
    "        parameters = backprop(y_hat, y, caches, parameters, learning_rate)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss: {}\".format(cross_entropy(y_hat, y)))\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(y_hat[0, :])\n",
    "            print(y_hat[50, :])\n",
    "            print(y_hat[149, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = pd.read_csv('resources/datasets/iris.csv')\n",
    "species = iris_dataset[['species']].values\n",
    "\n",
    "x = normalize(iris_dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values)\n",
    "y = []\n",
    "for s in species.T[0]:\n",
    "    if s == 'setosa':\n",
    "        y.append([1, 0, 0])\n",
    "    elif s == 'versicolor':\n",
    "        y.append([0, 1, 0])\n",
    "    elif s == 'virginica':\n",
    "        y.append([0, 0, 1])\n",
    "y = np.array(one_hot)\n",
    "\n",
    "train(x, y, 5000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
